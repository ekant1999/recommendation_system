# -*- coding: utf-8 -*-
"""Final_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bzZnPKwos7eQX_w4vWrGwLLIcY2iTzWX
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

Test_data=pd.read_csv('drive/My Drive/Colab Notebooks/test_data.csv')
Combined_data=pd.read_csv('drive/My Drive/Colab Notebooks/combined_csv.csv')
Train_data=pd.read_csv('drive/My Drive/Colab Notebooks/train_data.csv')

Combined_data

Test=Test_data["CustomerID"].values

Test

Train=Train_data["CustomerID"].values

Train

Test_data['Values']=Test_data['Quantity']*Test_data['UnitPrice']
Train_data['Values']=Train_data['Quantity']*Train_data['UnitPrice']
Combined_data['Values']=Combined_data['Quantity']*Combined_data['UnitPrice']

Combined_data['Values']

Combined_data=Combined_data.drop(['UnitPrice','InvoiceDate','InvoiceNo','Country','Quantity'],axis=1)
Test_data=Test_data.drop(['UnitPrice','InvoiceDate','InvoiceNo','Country','Quantity'],axis=1)

Combined_data = Combined_data.assign(StockID=(Combined_data['StockCode']).astype('category').cat.codes)

Combined_data = Combined_data.assign(NewCustomerID=(Combined_data['CustomerID']).astype('category').cat.codes)

Combined_data=Combined_data.sort_values('StockID')
Combined_data

stock_code=Combined_data.copy()

stock_code.shape

stock_code.head()

stock_code=stock_code.drop(['Values','CustomerID','NewCustomerID'],axis=1)

stock_code=stock_code.reset_index(drop=True)

stock_code=stock_code.drop_duplicates()

stock_code=stock_code.sort_values('StockID')
stock_code.head()

stock_code.shape

Combined_data=Combined_data.groupby(['CustomerID','StockID'],as_index=False).sum()



df_matrix = pd.pivot_table(Combined_data, values='Values', index='CustomerID', columns='StockID')
df_matrix

df_matrix_norm = df_matrix

#df_matrix_norm = (df_matrix_norm["Values"]-df_matrix_norm["Values"].min()) / (df_matrix_norm["Values"].max()-df_matrix_norm["Values"].min())
#df_matrix_norm = (df_matrix_norm-df_matrix_norm.min()) / (df_matrix_norm.max()-df_matrix_norm.min())
df_matrix_norm = (df_matrix_norm - df_matrix_norm.min()) / (df_matrix_norm.max() - df_matrix_norm.min())

df_matrix_norm=df_matrix_norm.fillna(0)
df_matrix_norm.head()

df_matrix_array=np.asarray(df_matrix)
df_matrix_array

df_new1=df_matrix_norm[df_matrix_norm.index.isin(Test)]

df_new2=df_matrix_norm[df_matrix_norm.index.isin(Train)]

df_new1.head()

df_new1=df_new1.stack().reset_index(name='Values')
df_new1.shape

df_new1=df_new1.apply(pd.to_numeric)

df_new1.head(5)

df_new1=df_new1.sort_values('Values')

df_new1.dtypes

#Merging movies_df with ratings_df by MovieID
merged_data_test = stock_code.merge(df_new1, on='StockID')
#Dropping unecessary columns
#Displaying the result
merged_data_test.head()

merged_data_test.shape

df_new2.head()

df_new2=df_new2.stack().reset_index(name='Values')
df_new2.shape

df_new2.sort_values(by='Values', ascending=False)

df_new2.head()

#Merging movies_df with ratings_df by MovieID
merged_data_train = stock_code.merge(df_new2, on='StockID')
#Dropping unecessary columns
#Displaying the result
merged_data_train.head()

merged_data_train.shape

df_matrix_norm=df_matrix_norm.stack().reset_index(name='Values')
df_matrix_norm.shape

#Merging movies_df with ratings_df by MovieID
merged_data_combine = stock_code.merge(df_matrix_norm, on='StockID')
#Dropping unecessary columns
#Displaying the result
merged_data_combine.head()

merged_data_combine.shape

df_matrix_norm.head()



merged_data_train.sort_values(by='Values', ascending=False)

#Group up by UserID
userGroup = merged_data_train.groupby('CustomerID')
userGroup.first().head()

userGroup.dtypes

set([ type(x) for x in userGroup ])

#Amount of users used for training
amountOfUsedUsers = 3751920
#Creating the training list
train_X = []
#For each user in the group
for userID, curUser in userGroup:
    #Create a temp that stores every movie's rating
    temp = [0]*len(stock_code)
    #For each movie in curUser's movie list
    for num, movie in curUser.iterrows():
        #Divide the rating by 5 and store it
        temp[movie['StockID']] = movie['Values']
    #Now add the list of ratings into the training list
    train_X.append(temp)
    #Check to see if we finished adding in the amount of users for training
    if amountOfUsedUsers == 0:
        break 
    amountOfUsedUsers -= 1

train_X

inputUser = [train_X[90]]

#Feeding in the user and reconstructing the input
hh0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)
vv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)
feed = sess.run(hh0, feed_dict={ v0: inputUser, W: prv_w, hb: prv_hb})
rec = sess.run(vv1, feed_dict={ hh0: feed, W: prv_w, vb: prv_vb})

scored_movies_df_75 = stock_code
scored_movies_df_75["Recommendation Score"] = rec[0]
scored_movies_df_75.sort_values(["Recommendation Score"], ascending=False).head(20)

merged_data_train.iloc[90]

stock_data_75 = merged_data_train[merged_data_train['CustomerID']==47259]
stock_data_75

#Merging movies_df with ratings_df by MovieID
merged_data_75 = scored_movies_df_75.merge(stock_data_75, on='StockID', how='outer')

merged_data_75.sort_values(["Recommendation Score"], ascending=False)

w=merged_data_75.loc[(merged_data_75['Values'] == 0.0)]

w

w=w.sort_values('Recommendation Score')
w

w["Recommendation Score"].idxmax()

"""merged_data_75["Recommendation Score"].idxmax()"""